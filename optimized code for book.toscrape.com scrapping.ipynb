{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9711eed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all required libraries for scrapping data\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6ef5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an empty list in which we will append all the url of each notebook page\n",
    "list_of_url = []\n",
    "\n",
    "for j in range(1,51):\n",
    "    # this is dynamic link for each page of book\n",
    "    # on each page there are 20 book and there are 50 such pages so we have taken\n",
    "    # for j in range(1,51)\n",
    "    link = 'https://books.toscrape.com/catalogue/page-'+str(j)+'.html'\n",
    "    # now creating a soup for each page to get all the html from it\n",
    "    soup = BeautifulSoup(requests.get(link).text)\n",
    "    \n",
    "    soup.find(class_ = 'col-xs-6 col-sm-4 col-md-3 col-lg-3')\n",
    "    # this is base url which remains same for each page of book \n",
    "    baseURL = 'https://books.toscrape.com/catalogue/'\n",
    "\n",
    "    for i in range(20):\n",
    "        # here we need to link of each page so it is basically combination of baseURL and the href obtained from soup \n",
    "        # so we append each url in the list to get output\n",
    "        list_of_url.append(baseURL+soup.find_all(class_ = 'col-xs-6 col-sm-4 col-md-3 col-lg-3')[i].find('a',href = True)['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1a13e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need all below listed basic information like UID, name_of_book, genre of book, rating on the book, \n",
    "# no of reviews and price of book\n",
    "# creating a empty list for all the features required for data\n",
    "UID = []\n",
    "book_name = []\n",
    "genre = []\n",
    "ratings = []\n",
    "no_of_review = []\n",
    "price = []\n",
    "# taking each link from list 1000 of url obtained in above cell \n",
    "for url in list_of_url:\n",
    "    # making soup of each url\n",
    "    soup = BeautifulSoup(requests.get(url).text)\n",
    "    # after understanding how the data is scrapped on single page\n",
    "    # below items are found in a loop for each book\n",
    "    UID.append(soup.find('table', class_ = 'table table-striped').find('td').text)\n",
    "    book_name.append(soup.find('ul',class_ = 'breadcrumb').find_all('li')[-1].text.strip())\n",
    "    genre.append(soup.find('ul',class_ = 'breadcrumb').find_all('li')[2].text.strip())\n",
    "    ratings.append(str(soup.find('div', class_ = 'col-sm-6 product_main').find('p',class_ = 'star-rating')).split()[2].replace('\">',''))\n",
    "    no_of_review.append(soup.find('table', class_ = 'table table-striped').find_all('td')[-1].text)\n",
    "    price.append(soup.find('table', class_ = 'table table-striped').find_all('td')[3].text.replace('Â£',''))\n",
    "    \n",
    "# after completing all the iterations(~1000) we get 6 list of features required\n",
    "# with 1000 items each for each book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039bd9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating data frame out of list \n",
    "\n",
    "df = pd.DataFrame({'UID' : UID, 'book_name' : book_name, 'genre' : genre, 'ratings' : ratings, 'no_of_reviews' : no_of_review,\n",
    "                  'price' : price})\n",
    "\n",
    "# and exporting the file as csv to system for furthur processing\n",
    "df.to_csv('file path to your computer/file_name.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
